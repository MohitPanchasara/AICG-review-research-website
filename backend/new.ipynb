{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de951aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel(encoder\u001b[38;5;241m=\u001b[39mViTModel(vit_cfg), decoder\u001b[38;5;241m=\u001b[39mGPT2LMHeadModel(gpt2_cfg))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ==== 2) Load your .pth safely (weights_only=True) ====\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m sd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVITGPT2_PTH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sd, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sd: sd \u001b[38;5;241m=\u001b[39m sd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     46\u001b[0m fixed \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\mohit\\Desktop\\PROJECTS\\AI Image Detection\\resnet-50\\resnet_env_gpu\\lib\\site-packages\\torch\\serialization.py:1524\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1516\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1517\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1518\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1521\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1522\u001b[0m                 )\n\u001b[0;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1526\u001b[0m             opened_zipfile,\n\u001b[0;32m   1527\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1531\u001b[0m         )\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.dtype was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.dtype])` or the `torch.serialization.safe_globals([numpy.dtype])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# --- ONE-CELL OFFLINE VIT-GPT2 INFERENCE (JSON TIMELINE) ---\n",
    "# Edit these 4 paths and run. No downloads. Uses a single .pth + local tokenizer.\n",
    "VITGPT2_PTH         = r\"weights\\vitgpt2\\vitgpt2.pth\"      # your VisionEncoderDecoder state_dict\n",
    "TOKENIZER_DIR       = r\"weights\\vitgpt2\\tokenizer\"        # must contain vocab.json + merges.txt\n",
    "VIDEO_PATH          = r\"..\\..\\..\\..\\sample_original.mp4\"           # offline video to summarize\n",
    "OUTPUT_JSON_PATH    = r\"summary_output.json\"              # where to save the JSON\n",
    "\n",
    "# ---- Minimal imports (keep it light) ----\n",
    "import os, json, time, cv2, numpy as np, torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.serialization import add_safe_globals\n",
    "from transformers import (ViTConfig, ViTModel, GPT2Config, GPT2LMHeadModel,\n",
    "                          VisionEncoderDecoderModel, GPT2TokenizerFast)\n",
    "\n",
    "# ---- Safety for torch.load in PyTorch 2.6+ (allowlist numpy scalar) ----\n",
    "add_safe_globals([np.core.multiarray.scalar])  # safe to allow if you trust the file\n",
    "\n",
    "# ---- Basic checks ----\n",
    "assert os.path.exists(VITGPT2_PTH), f\"Missing: {VITGPT2_PTH}\"\n",
    "assert os.path.isfile(os.path.join(TOKENIZER_DIR, \"vocab.json\")), \"Missing tokenizer vocab.json\"\n",
    "assert os.path.isfile(os.path.join(TOKENIZER_DIR, \"merges.txt\")), \"Missing tokenizer merges.txt\"\n",
    "assert os.path.exists(VIDEO_PATH), f\"Missing video: {VIDEO_PATH}\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4)//2))\n",
    "\n",
    "# ==== 1) Build ViT-GPT2 architecture in code (no config.json / downloads) ====\n",
    "# If you trained ViT-B/16, set PATCH_SIZE=16. For ViT-B/32, keep 32.\n",
    "IMAGE_SIZE, PATCH_SIZE = 224, 32\n",
    "HIDDEN_SIZE, NUM_LAYERS, NUM_HEADS, INTERMEDIATE_SIZE = 768, 12, 12, 3072\n",
    "\n",
    "vit_cfg = ViTConfig(image_size=IMAGE_SIZE, patch_size=PATCH_SIZE,\n",
    "                    hidden_size=HIDDEN_SIZE, num_hidden_layers=NUM_LAYERS,\n",
    "                    num_attention_heads=NUM_HEADS, intermediate_size=INTERMEDIATE_SIZE)\n",
    "gpt2_cfg = GPT2Config(n_positions=1024, n_ctx=1024, n_embd=HIDDEN_SIZE,\n",
    "                      n_layer=NUM_LAYERS, n_head=NUM_HEADS,\n",
    "                      bos_token_id=50256, eos_token_id=50256,\n",
    "                      is_decoder=True, add_cross_attention=True, use_cache=True)\n",
    "\n",
    "model = VisionEncoderDecoderModel(encoder=ViTModel(vit_cfg), decoder=GPT2LMHeadModel(gpt2_cfg))\n",
    "\n",
    "# ==== 2) Load your .pth safely (weights_only=True) ====\n",
    "sd = torch.load(VITGPT2_PTH, map_location=\"cpu\", weights_only=True)\n",
    "if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n",
    "fixed = {}\n",
    "for k, v in sd.items():\n",
    "    nk = k\n",
    "    for pref in (\"module.\", \"model.\"):\n",
    "        if nk.startswith(pref): nk = nk[len(pref):]\n",
    "    fixed[nk] = v\n",
    "model.load_state_dict(fixed, strict=False)\n",
    "model.to(device).eval()\n",
    "for p in model.parameters(): p.requires_grad = False\n",
    "\n",
    "# ==== 3) Tokenizer from local files only (no downloads) ====\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(TOKENIZER_DIR, local_files_only=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "# ==== 4) Minimal preprocessing & caption helpers ====\n",
    "# If you trained with ImageNet stats, swap Normalize to mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "])\n",
    "GEN = dict(max_new_tokens=16, do_sample=False, num_beams=3)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def caption_frame(rgb):\n",
    "    px = tfm(Image.fromarray(rgb)).unsqueeze(0).to(device)\n",
    "    ids = model.generate(pixel_values=px,\n",
    "                         pad_token_id=tokenizer.pad_token_id,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         **GEN)\n",
    "    return tokenizer.decode(ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def video_meta(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    fps = float(cap.get(cv2.CAP_PROP_FPS) or 0.0)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    dur = (total / fps) if (fps > 0 and total > 0) else 0.0\n",
    "    cap.release()\n",
    "    return fps, total, dur\n",
    "\n",
    "def frame_at_time(path, t):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    fps = float(cap.get(cv2.CAP_PROP_FPS) or 0.0)\n",
    "    if fps <= 0:\n",
    "        ok, frm = cap.read(); cap.release()\n",
    "        if not ok: raise RuntimeError(\"Could not read frame\")\n",
    "        return frm\n",
    "    idx = int(round(t * fps))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, idx))\n",
    "    ok, frm = cap.read(); cap.release()\n",
    "    if not ok or frm is None: raise RuntimeError(f\"Could not read frame at {t:.2f}s\")\n",
    "    return frm\n",
    "\n",
    "# ==== 5) Summarize one offline video into your JSON format ====\n",
    "def summarize_video_json(path, window_sec=3.0, stride_sec=1.0, max_windows=16):\n",
    "    _, _, dur = video_meta(path)\n",
    "    if dur <= 0: dur = 6.0\n",
    "    starts = np.arange(0.0, max(0.0, dur-1e-6), stride_sec)\n",
    "    items, n = [], 0\n",
    "    t0 = time.perf_counter()\n",
    "    for s in starts:\n",
    "        if n >= max_windows: break\n",
    "        start, end = float(s), float(min(s+window_sec, dur))\n",
    "        center = (start + end) / 2.0\n",
    "        bgr = frame_at_time(path, center)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        text = caption_frame(rgb)\n",
    "        items.append([round(start,2), round(end,2), text])\n",
    "        n += 1\n",
    "    print(f\"Segments: {len(items)} | Elapsed: {time.perf_counter()-t0:.2f}s on {device}\")\n",
    "    return {\"items\": items, \"duration\": round(dur, 2)}\n",
    "\n",
    "# ==== 6) Run & print/save JSON ====\n",
    "result = summarize_video_json(VIDEO_PATH, window_sec=3.0, stride_sec=1.0, max_windows=16)\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON_PATH) or \".\", exist_ok=True)\n",
    "with open(OUTPUT_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved JSON to:\", OUTPUT_JSON_PATH)\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# ---- If (and only if) safe load still fails, you can fall back to:\n",
    "# sd = torch.load(VITGPT2_PTH, map_location=\"cpu\", weights_only=False)  # ONLY for trusted files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c7105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resnet_env_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
